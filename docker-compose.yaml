# --- Common Airflow Configuration Blocks ---
x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.1}
  user: "${AIRFLOW_UID}:0"
  volumes: &airflow-common-volumes # Essential shared directories
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    # Shared volume for Spark event logs (Airflow History Server)
    - spark_events:/opt/spark/events:rw
    # Mount Spark Jars to Airflow containers so it can see them for SparkSubmitOperator
    - ./spark-jobs:/opt/bitnami/spark/jobs
  environment: &airflow-common-env
    AIRFLOW_UID: ${AIRFLOW_UID}
    AIRFLOW_GID: ${AIRFLOW_GID}
    AIRFLOW_HOME: /opt/airflow
    # Configuration for Airflow to connect to its metadata DB
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_USER:-airflow}:${AIRFLOW_PASSWORD:-airflow}@postgres-airflow/${AIRFLOW_DB:-airflow}
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__WEBSERVER__RBAC: "true"
    # Spark Connection details (Airflow will use this to connect to spark-master)
    AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
    # MinIO/S3 Connection details for Airflow operators
    AIRFLOW_CONN_AWS_DEFAULT: '{"conn_type": "aws", "host": "http://minio:9000", "login": "minioadmin", "password": "minioadminpassword", "extra": {"aws_access_key_id": "minioadmin", "aws_secret_access_key": "minioadminpassword", "endpoint_url": "http://minio:9000", "region_name": "us-east-1", "s3_verify": false}}'
  extra_hosts:
    - "host.docker.internal:host-gateway"

# --- SERVICE DEFINITIONS ---
services:
  # --- 1. Airflow PostgreSQL (Metadata DB) ---
  postgres-airflow:
    image: postgres:13
    container_name: postgres_airflow_meta
    environment:
      POSTGRES_USER: ${AIRFLOW_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_DB:-airflow}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data/pgdata
    ports:
      - "5433:5432"
    restart: unless-stopped

  # --- 2. Airflow Initializer (THE MISSING SERVICE) ---
  airflow-init:
    <<: *airflow-common # Inherit common config
    container_name: airflow_init
    entrypoint: ["/usr/bin/dumb-init", "--", "/entrypoint.sh", "db", "init"]
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    user: "${AIRFLOW_UID}:0" # Set correct permissions
    depends_on:
      - postgres-airflow

  # --- 3. Airflow Webserver (UI on :8080) ---
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    depends_on:
      - airflow-init # Must wait for init to complete
      - spark-master
    ports:
      - "8080:8080"
    entrypoint: ["/usr/bin/dumb-init", "--", "/entrypoint.sh", "webserver"]
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    restart: always

  # --- 4. Airflow Scheduler ---
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    depends_on:
      - airflow-init # Must wait for init to complete
      - spark-master
    volumes: *airflow-common-volumes
    entrypoint: ["/usr/bin/dumb-init", "--", "/entrypoint.sh", "scheduler"]
    environment: *airflow-common-env
    restart: always

  # --- 5. Your Destination Database (PostgreSQL/Redshift Mock) ---
  postgres:
    image: postgres:16-alpine
    container_name: postgres_db
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: $POSTGRES_USER
      POSTGRES_PASSWORD: $POSTGRES_PASSWORD
      POSTGRES_DB: $POSTGRES_DB
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  # --- 6. MinIO Service (S3-Compatible Data Lake) ---
  minio:
    image: minio/minio
    container_name: minio_storage
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadminpassword
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    restart: unless-stopped

  # --- 7. Spark Master Service (Scala ETL Engine) ---
  spark-master:
    image: apache/spark:3.5.1 # Specific and stable tag for the official image
    container_name: spark_master
    ports:
      - "7077:7077" # Spark Master Port for Worker communication and Airflow connection
      - "8081:8080" # Spark Web UI (Mapped to 8081 to avoid Airflow conflict)
    # EXPLICIT command required for the official Apache image to start the Master daemon
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      # No SPARK_MODE needed for official image, but we need to set the log directory
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: /opt/spark/events
    volumes:
      # Shared volumes are essential and were previously hidden by ...other_config...
      - ./spark-jobs:/opt/spark/jobs # Mount your Scala job JARs
      - spark_events:/opt/spark/events # Shared volume for Spark logs (for history server and Airflow)
    restart: unless-stopped

  # --- 8. Spark Worker Service (Official Image) ---
  spark-worker:
    image: apache/spark:3.5.1 # Specific and stable tag for the official image
    container_name: spark_worker
    # Must depend on master to ensure the cluster entry point exists
    depends_on:
      - spark-master
    # EXPLICIT command required for the official Apache image to start the Worker daemon
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      # Worker needs to know where the master is
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2 # Adjust based on your laptop's resources
      SPARK_WORKER_MEMORY: 2g # Adjust based on your laptop's resources
    volumes:
      # Workers don't need the jobs, but they often need the logs and scratch space
      - spark_events:/opt/spark/events
    restart: unless-stopped

  # --- 9. Trino Service (Athena Alternative: Distributed SQL) ---
  trino:
    image: trinodb/trino:latest
    container_name: trino_query_engine
    ports:
      - "8082:8080"
    volumes:
      - ./trino/etc:/etc/trino
    depends_on:
      - minio
      - postgres
    restart: unless-stopped

  # --- 10. Ollama Service (LLM Server) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

# --- Persistent Volumes ---
volumes:
  postgres_data:
  ollama_models:
  minio_data:
  postgres-airflow-data:
  spark_events:
