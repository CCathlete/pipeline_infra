# ./docker-compose-airflow-spark-trino.yaml

# --- Common Airflow Configuration Blocks ---
x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.1}
  user: "${AIRFLOW_UID}:0"
  volumes: &airflow-common-volumes # Essential shared directories
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    # Shared volume for Spark event logs (Airflow History Server)
    - spark_events:/opt/spark/events:rw
    # Mount Spark Jars to Airflow containers so it can see them for SparkSubmitOperator
    - ./spark-jobs:/opt/bitnami/spark/jobs
  environment: &airflow-common-env
    AIRFLOW_UID: ${AIRFLOW_UID}
    AIRFLOW_GID: ${AIRFLOW_GID}
    AIRFLOW_HOME: /opt/airflow
    # Configuration for Airflow to connect to its metadata DB
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${POSTGRES_DB}
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__WEBSERVER__RBAC: "true"
    # Spark Connection details (Airflow will use this to connect to spark-master)
    AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
    # MinIO/S3 Connection details for Airflow operators
    AIRFLOW_CONN_AWS_DEFAULT: '{"conn_type": "aws", "host": "http://minio:9000", "login": "minioadmin", "password": "minioadminpassword", "extra": {"aws_access_key_id": "minioadmin", "aws_secret_access_key": "minioadminpassword", "endpoint_url": "http://minio:9000", "region_name": "us-east-1", "s3_verify": false}}'
  extra_hosts:
    - "host.docker.internal:host-gateway"
  networks:
    - my_shared_network

# --- SERVICE DEFINITIONS ---
services:
  # --- 2. Airflow Initializer (THE MISSING SERVICE) ---
  airflow-init:
    <<: *airflow-common # Inherit common config
    container_name: airflow_init
    command: ["db", "init"]
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    # user: "${AIRFLOW_UID}:0" # Set correct permissions
    depends_on:
      - postgres

  # --- 3. Airflow Webserver (UI on :8080) ---
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    depends_on:
      - airflow-init # Must wait for init to complete
      - spark-master
    ports:
      - "8080:8080"
    command: ["webserver"]

    #Not sure it's working, command to create login creds automatically.

    # command: bash -c "airflow db init && airflow users create --username ${AIRFLOW_USER:-airflow} --firstname Admin --lastname User --role Admin --email admin@example.com --password ${AIRFLOW_PASSWORD:-airflow}"

    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    restart: always

  # --- 4. Airflow Scheduler ---
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    depends_on:
      - airflow-init # Must wait for init to complete
      - spark-master
    volumes: *airflow-common-volumes
    command: ["scheduler"]
    environment: *airflow-common-env
    restart: always

  # --- 5. Your Destination Database (PostgreSQL/Redshift Mock) ---
  postgres:
    image: postgres:16-alpine
    container_name: postgres_db
    ports:
      - "${POSTGRES_PORT}:5432"
    environment:
      POSTGRES_USER: $POSTGRES_USER
      POSTGRES_PASSWORD: $POSTGRES_PASSWORD
      POSTGRES_DB: $POSTGRES_DB
      POSTGRES_PORT: $POSTGRES_PORT
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
  networks:
    - my_shared_network

  # --- 6. MinIO Service (S3-Compatible Data Lake) ---
  minio:
    image: minio/minio
    container_name: minio_storage
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadminpassword
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    restart: unless-stopped
    networks:
      - my_shared_network

  # --- 7. Spark Master Service (Scala ETL Engine) ---
  spark-master:
    image: apache/spark:3.5.1 # Specific and stable tag for the official image
    container_name: spark_master
    ports:
      - "7077:7077" # Spark Master Port for Worker communication and Airflow connection
      - "8081:8080" # Spark Web UI (Mapped to 8081 to avoid Airflow conflict)
    # EXPLICIT command required for the official Apache image to start the Master daemon
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      # No SPARK_MODE needed for official image, but we need to set the log directory
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: /opt/spark/events
    volumes:
      # Shared volumes are essential and were previously hidden by ...other_config...
      - ./spark-jobs:/opt/spark/jobs # Mount your Scala job JARs
      - spark_events:/opt/spark/events # Shared volume for Spark logs (for history server and Airflow)
    restart: unless-stopped
    networks:
      - my_shared_network

  # --- 8. Spark Worker Service (Official Image) ---
  spark-worker:
    image: apache/spark:3.5.1 # Specific and stable tag for the official image
    container_name: spark_worker
    # Must depend on master to ensure the cluster entry point exists
    depends_on:
      - spark-master
    # EXPLICIT command required for the official Apache image to start the Worker daemon
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      # Worker needs to know where the master is
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2 # Adjust based on your laptop's resources
      SPARK_WORKER_MEMORY: 2g # Adjust based on your laptop's resources
    volumes:
      # Workers don't need the jobs, but they often need the logs and scratch space
      - spark_events:/opt/spark/events
    restart: unless-stopped
    networks:
      - my_shared_network

  # --- 9. Trino Service (Athena Alternative: Distributed SQL) ---
  trino:
    image: trinodb/trino:latest
    container_name: trino_query_engine
    ports:
      - "8082:8080"
    user: "1000:1000"
    volumes:
      - ./trino/etc:/etc/trino
      - ./trino_data:/var/lib/trino
    entrypoint: ["/usr/lib/trino/bin/run-trino"]
    depends_on:
      - minio
    restart: unless-stopped
    networks:
      - my_shared_network

  # --- 10. Ollama Service (LLM Server) ---
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    networks:
      - my_shared_network

networks:
  my_shared_network:
    name: my_shared_network

# --- Persistent Volumes ---
volumes:
  postgres_data:
  ollama_models:
  minio_data:
  spark_events:
